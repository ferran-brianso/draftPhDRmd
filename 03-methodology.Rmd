---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/template.tex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
#bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

```{block type='savequote', quote_author='(ref:faust-quote)', include=knitr::is_latex_output()}
Ein Mann, der recht zu wirken denkt,

Mu\ss\enspace auf das beste Werkzeug halten


_The man who seeks to be approved,_

_must stick to the best tools for it_
```
(ref:faust-quote) --- Goethe's *Faust. Eine Tragödie* (1808).

<!-- 
Notes for adding an opening quote in PDF output:
i) add the reference for the quote with the chunk option quote_author="my author name",
ii) include=knitr::opts_knit$get('rmarkdown.pandoc.to') == 'latex' means that these quotes are only included when output is latex (in HTML output, it would appear by the end of the previous page)
iii) You can't use markdown syntax inside chunk options, so if you want to e.g. italicise a book name in the quote reference use a 'text reference': Create a named piece of text with '(ref:label-name) My text', then link to this in the chunk option with quote_author='(ref:label-name)'
-->

# Methodology {#methods}
\chaptermark{Methodology}
\minitoc <!-- this will include a mini table of contents-->

In the context of multi-omics data integration, our proposal relies on the idea that incorporating biological annotations into datasets before integrative analysis enriches outcomes and enhances their biological interpretability. Therefore, augmenting quantitative omics data with contextual biological knowledge will deepen our understanding of complex biological phenomena. To do so, we begin with meticulous data quality assessment and standardization, laying the foundation for reliable analyses. We then infuse biological knowledge using standard biological annotations, creating "Expanded Datasets" that provide context for comprehensive analysis. Advanced dimension-reduction techniques can be applied to illuminate hidden patterns and relationships between data sources or blocks, and the semi-automation capabilities of the Targets R package allow us to build an easy-to-use implementation of the process.


## Data Quality Assessment and Format Review {#qa}

Before initiating the integrative analysis, a meticulous evaluation of data quality and format compatibility was conducted to ensure the reliability of the input datasets. This crucial step aimed to identify and rectify discrepancies, inconsistencies, or errors that could potentially impact subsequent analyses. During this process, datasets spanning various omics technologies, including transcriptomics and proteomics, are selectively acquired from reputable sources and repositories. Emphasis was placed on meticulous source selection to guarantee consistency and adherence to standardized formats. Subsequently, the raw omics data underwent a comprehensive preprocessing phase, addressing issues such as missing values, outliers, and normalization. This preprocessing step was indispensable for enhancing data quality and enabling comparability across diverse datasets. Additionally, a thorough review of data formats encompassing file types, column naming conventions, and units of measurement was conducted. Non-standardized data were systematically transformed into a uniform format to streamline the downstream integration processes. Through these procedures, a robust foundation was established for subsequent integrative omics analysis, ensuring coherence and validity of the synthesized biomedical insights.


\bigskip
FALTA DETALLAR AQUI COM SHAN VALIDAT ELS DATASETS DE TCGA (ELS DE STROKE ANIRAN A BANDA, pero fer-hi mencio breu).

Explicar aquí els requeriments de format dels data sets d'entrada

Mostrar Figure \@ref(fig:fig3-10) i Figure \@ref(fig:fig3-9) PERO POTSER MILLOR COM A TAULES INTEGRADES AMB MARKDOWN?


```{r fig3-10, fig.align='center', fig.scap="Example of gene and protein data loaded in R", fig.cap="Example of gene expression and protein quantification data as loaded in R", out.width="95%", echo=FALSE}
knitr::include_graphics("figures/chapter3/3-10_input_data_R_example.png")
```


```{r fig3-9, fig.align='center', fig.cap="Example of proteomics input data", out.width="95%", echo=FALSE}
knitr::include_graphics("figures/chapter3/3-9_protein_data_example.png")
```

\clearpage


## Preprocessing for Integration of Biological Knowledge: Generating the "Expanded Datasets" {#preprocess}

The integration of biological knowledge into omics datasets can be achieved through a preprocessing step aimed at expanding the original data matrices with annotations accessed from specialized R libraries, which provided direct access to curated biological databases such as Gene Ontology (GO[@ashburner_gene_2000]) and biochemical pathways information (e.g., KEGG[@kanehisa_kegg_2000]). This process, that combines the annotation of the most significant biological entities with the quantification and integration of their annotation values to the data matrices,  ends up with what we term "Expanded Data Sets", which include the original biological features (e.g., gene expression or protein quantification values) as well as new variables coming from the annotation of biological  terms. The following steps explain this preprocessing procedure in more detail:


* Selected biological annotations: Specialized R libraries, dedicated to biological knowledge integration, are employed to access and retrieve up-to-date annotations from databases such as GO and KEGG.


* Data-Annotation Mapping: Each omics dataset are mapped to the retrieved biological annotations based on identifiers (e.g., gene or protein names) using the capabilities of the R libraries. This step facilitates the linking of omics data to biological knowledge.


* Annotation Integration: The annotated information is integrated, implementing new R functions, into the starting omics datasets, resulting in expanded data matrices that combined the original quantitative omics measurements with new quantified features associated with the given biological annotations.

### Selecció de les fonts d'anotacions biològiques {#biosources}
PENDENT DE DETALLAR com escullo les fonts de les anotacions per defecte. 
Apuntar que es poden facilitar ja anotacions disponibles prèviament, sempre que compleixin amb el format que s'explica al següent apartat. 
Aquestes poden ser estàndard o bé personalitzades a mida de l'usuari (tot i que si es aixi hi ha certes funcionalitats posteriors que no es podran aprofitar).

\clearpage





### Anotació de la info biològica {#bioannotation}
COM VAM PLANTEJAR fer l'anotació biologica. Quines opcions i amb quins metodes estadístics/bioinformatics...
DUBTO SI LO QUE SEGUEIX NO ANIRIA A RESULTATS

For each input data set, if annotations are not already provided, two distinct basic annotation methods can be performed:

(i) a basic GO mapping, returning annotations to those GO entities for which we find more than a certain number of features (gene ids coming from our data set, see Figure \@ref(fig:fig3-8) for an example) annotated to them, 

(ii) a Gene Enrichment Analysis (based on Hypergeometric tests against all GO categories, with FDR correction) is performed in order to retrieve the most relevant annotations to that set of genes/features.[@yu_clusterprofiler_2012]

[mostrar exemple de llista de gens]

```{r fig3-8, fig.align='center', fig.cap="List of gene symbols used as example", out.width="95%", echo=FALSE}
knitr::include_graphics("figures/chapter3/3-8_gene_list.png")
```

[punt de millora, que l'anotacio basica pugui ser tb a KEGG]

[mostrar formula] 

es mostra exemple en Figure \@ref(fig:fig3-11) POSSIBLE INTEGRAT EN MARKDOWN?

```{r fig3-11, fig.align='center', fig.scap="Example of basic GO annotation by raw count", fig.cap="Example of basic Go annotation by raw count against GO Biological Processes, setting 8 as minimum number of genes included in the BP entity. Annotation performed separately for gene expression and protein quantification input files", out.width="95%", echo=FALSE}
knitr::include_graphics("figures/chapter3/3-11_basicGO_example.png")
```


[comentar aquí l'opció d'afegir les anotacions com a individus suplementaris enlloc de variables Figure \@ref(fig:fig3-1) is an example.]


```{r fig3-12, fig.align='center', fig.scap="Example of results from GO annotation", fig.cap="Example of results from GO annotation. Results of the biological significance analysis performed with the lists of genes against GO through clusterProfiler", out.width="95%", echo=FALSE}
knitr::include_graphics("figures/chapter3/3-12_clusterprofiler_results.png")
```

\clearpage


```{r fig3-1, fig.align='center', fig.cap="Addition of GO terms", out.width="95%", echo=FALSE}
knitr::include_graphics("figures/chapter3/3-1_addition_of_GO_terms.png")
```


Alternatively, manual annotations can be provided (eg. GO terms, canonical pathways, or even annotation to custom entities) as an optional input file. 

[mostrar el format requerit].

Other annotation methods can be implemented, as functions to be used by the main pipeline, if more complex methods for biological information addition are required.

[Mostrar el format final de les anotacions, com a matrius dels data sets amb anotacions binàries 1/0 com a columnes extra]


EXPANSIO DE LES MATRIUS (numeritzar anotacions, creació de noves vars a partir de les anotacions)

The process starts already having a couple of data sets from distinct 'omics sources [punt de millora: admetre 3 o + inputs, comentar més tard a Discussion], mapped to gene ids (in the default case, where GO annotation have been performed), containing the results from a selection of differentially expressed genes or most relevant proteins analysis, or similar.

\clearpage

### Annotation Integration {#biointegration}

Once the annotations are already computed, mapping each feature of the input data set to the corresponding biological entity, they can be used to generate new features (as new rows), computing the average value [punt de millora: funció de ponderació] of the expression/intensity values from all original features being mapped to the annotated biological entities.

```{r fig3-2, fig.align='center', fig.cap="Addition of news feats", out.width="95%", echo=FALSE}
knitr::include_graphics("figures/chapter3/3-2_addition_of_new_feats.png")
```

```{r fig3-3, fig.align='center', fig.cap="Gene enrichment diagram", out.width="95%", echo=FALSE}
knitr::include_graphics("figures/chapter3/3-3_gene_enrichment_diagram.png")
```

```{r fig3-4, fig.align='center', fig.cap="Matrix expansion diagram", out.width="95%", echo=FALSE}
knitr::include_graphics("figures/chapter3/3-4_matrix_expansion_diagram.png")
```

Once we have the annotated matrices (Figure \@ref(fig:fig3-4), highlighted in blue) we proceed to generate the Expanded matrices (in green) by casting these annotations as numerical values, that is, calculating the average of the numerical expressions of each individual for the variables annotated to each category. This is done with the matrix product of the initial numerical values (expression, proteins...) with the transposed matrices of their annotations, and then with the inverse matrix of a diagonal matrix of the count of how many annotations each category or entity annotated has had.


```{r fig3-5, fig.align='center', fig.cap="Addition of new feats (2)", out.width="95%", echo=FALSE}
knitr::include_graphics("figures/chapter3/3-5_addition_of_new_feats_2.png")
```

```{r fig3-6, fig.align='center', fig.cap="Matrix expansion diagram (2)", out.width="95%", echo=FALSE}
knitr::include_graphics("figures/chapter3/3-6_matrix_expansion_diagram_2.png")
```



## Integrative Analysis with Joint Dimension Reduction Techniques

To uncover meaningful insights from the expanded data sets and extract relevant information from the integrated omics and biological knowledge, contrasted joint dimension reduction techniques were employed. These techniques enable the simultaneous analysis of multiple data types and facilitate the identification of key patterns and relationships. The following methods were applied:

* Multiple Factor Analysis (MFA): MFA, adapted for multi-omics data, was utilized to identify sources of variability in the integrated dataset while considering both quantitative omics data and biological annotations. MFA aims to maximize relevant information within the data while accounting for the hierarchical structure of the biological knowledge.

* Multiple Co-Inertia Analysis (MCIA): MCIA, a technique that aligns the covariance structures of multiple datasets, was employed to explore relationships between omics measurements and biological annotations. MCIA seeks to identify common patterns and associations between these data sources.

* Regularized Generalized Canonical Correlation Analysis (RGCCA): RGCCA was used to identify latent variables that capture joint information from omics data and biological annotations. RGCCA extends canonical correlation analysis to handle multi-view data integration and helps reveal correlated features across datasets.

PUNTS A INCLOURE: 

* Reducció de dimensió. Anàlisi factorial en detall (MFA), + MCIA + RGCCA

* incloure aquí % variabilitat explicat segons la estructura de la intersecció de les 2 taules (article Lovino 2022)

* avantatge del MFA és que podem definir blocs de variables!

* no mirem unicament si guanyem variabilitat, sino tambe si millorem interpretabilitat biologica


## Semi-Automation using the Targets R Package

The semi-automation of the integrative analysis process was facilitated by leveraging the Targets R package, which provides an efficient and user-friendly framework for building and managing complex analysis pipelines. In the development of the Targets pipeline, careful management of functions and parameters was essential to ensure a systematic and reproducible workflow. The following principles were applied:

* Function Modularity: Functions within the Targets pipeline were designed to be modular, focusing on specific tasks or analyses. This modularity enhanced code readability and maintainability.

* Parameterization: Parameters for each function and analysis step were carefully defined, allowing for flexibility and adaptability in the pipeline. This parameterization enabled the adjustment of analysis settings without modifying the underlying code.

* Dependency Management: Dependencies between different analysis steps were explicitly defined within the pipeline. This ensured that each step was executed in the correct order, and dependencies were automatically managed by the Targets package.

* Error Handling: Error handling procedures were implemented to capture and address potential issues during pipeline execution. This included the ability to handle errors, retries, and reporting of errors for troubleshooting. (NO APLICAT ARA PER ARA!)
 
PENDENT A AMPLIAR:

* Introduccio al paquet Targets en general i de les seves caracteristiques...

The R 'targets' package is a powerful tool for building and managing data science and data analysis pipelines. It is primarily designed for workflow automation, dependency management, and parallel processing in R projects. This package is useful for the following purposes:

1. Define and Manage Workflows: You can create a directed acyclic graph (DAG) that represents the workflow of your data analysis or machine learning project. Each node in the graph corresponds to a target, which can be a data file, an R script, or any other computational task.

2. Manage Dependencies: 'targets' allows you to specify dependencies between targets, ensuring that tasks are executed in the correct order. If a target depends on another target, it won't be executed until its dependencies are up-to-date.

3. Parallel Processing: One of the strengths of 'targets' is its ability to parallelize tasks. It can automatically determine which targets can be executed concurrently, improving the efficiency of your workflows, especially when working with large datasets or computationally intensive tasks.

4. Incremental Builds: When you make changes to your code or data, 'targets' can identify the minimal set of targets that need to be recomputed, saving time and computational resources. This is particularly useful for iterative development and experimentation.

5. Reports and Logging: 'targets' provides tools for generating reports and logging the progress of your workflow, making it easier to track and document your work.

6. Integration: It can be seamlessly integrated with other R packages and tools, such as 'drake' for more advanced data workflow management.

So, the 'targets' package is especially valuable for projects where data processing is a significant component, and you need a structured way to manage the various steps of your analysis or modeling pipeline. It helps ensure that your analyses are reproducible, efficient, and well-documented.

* Sistema que hem aplicat per crear el pipeline amb Targets...

```{r fig3-7, fig.align='center', fig.cap="Workflow overview", out.width="95%", echo=FALSE}
knitr::include_graphics("figures/chapter3/3-7_workflow_overview.png")
```

Targets workflow diagram (Figure \@ref(fig:fig3-7)) showing the steps corresponding with the complete process: The pipeline starts from (A) a couple of ‘omics-derived input data sets (e.g. pre-processed gene expression and protein abundance matrices). These are converted to R data frames with features in rows and samples in columns. Then, a data frame containing related annotations (B) is created, or loaded, for each given input matrix, and used to expand these original data, in order to end up with a pair of data frames (C) containing the original values plus the average expression/abundance values of the features related to each annotation as new features in additional rows. After that, distinct Dimension Reduction Methods are applied to perform the integrative analysis (D), and finally, an R markdown report (E) is rendered to show steps and main results of the full process.
